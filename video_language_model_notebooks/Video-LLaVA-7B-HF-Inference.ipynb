{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28242582-3c7b-4bdf-9f05-32208a32aad4",
   "metadata": {},
   "source": [
    "Code Reference: https://huggingface.co/LanguageBind/Video-LLaVA-7B-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfff3c4d-048e-4d4d-a91f-554f9332b75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stable_env\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(os.getenv(\"CONDA_DEFAULT_ENV\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adb83954-7e7d-4895-86da-16695f613960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from transformers import VideoLlavaForConditionalGeneration, VideoLlavaProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72537662-d40a-4122-af52-05e936cc6622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e07691f-e70e-4805-8967-c104f865f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`list[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b458aa85-7067-4537-a6f8-111117202ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fe571f4-ca52-4ff4-97c2-8012d2999013",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01334eb1-4942-4fe0-81f3-963116814d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767b04d3394d475187f99efa6d74cc3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5896b876a12446759a92977f80febad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed9a1dc343443f285bfa93fdd8fec44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model in half-precision\n",
    "model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "    \"LanguageBind/Video-LLaVA-7B-hf\", \n",
    "    dtype=torch.float16, \n",
    "    device_map=\"auto\", \n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "processor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be993ee2-d1dc-4d97-9de0-a7a0b2c951ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoLlavaForConditionalGeneration(\n",
       "  (model): VideoLlavaModel(\n",
       "    (video_tower): CLIPVisionModel(\n",
       "      (vision_model): CLIPVisionTransformer(\n",
       "        (embeddings): CLIPVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "          (position_embedding): Embedding(257, 1024)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (image_tower): CLIPVisionModel(\n",
       "      (vision_model): CLIPVisionTransformer(\n",
       "        (embeddings): CLIPVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "          (position_embedding): Embedding(257, 1024)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (multi_modal_projector): VideoLlavaMultiModalProjector(\n",
       "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (act): GELUActivation()\n",
       "      (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "    (language_model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32064, 4096, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499d96b-3be3-4099-a540-c585c673433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the video as an np.arrau, sampling uniformly 8 frames\n",
    "from huggingface_hub import hf_hub_download\n",
    "# video_path = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename='/home/aritrad/video-study/INTRO.mp4', repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb0fc73-b323-4555-a159-85867d177d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cae03323-7538-4de9-8ece-343c8c9b63e9",
   "metadata": {},
   "source": [
    "### Set Video Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c2497-6678-45ab-a0d3-fefeb237d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '/home/aritrad/MSR-Project/samples/4min-video.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5efc95-d05c-4476-a33a-a76ec34532f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = av.open(video_path)\n",
    "total_frames = container.streams.video[0].frames\n",
    "indices = np.arange(0, total_frames, total_frames / 16).astype(int)\n",
    "video = read_video_pyav(container, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2130b6a8-c180-4112-b9fd-61f5c4c27550",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1303c-447a-49e9-8e47-ab7c3e8d2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098fed3-d36d-4477-8c14-4ba93cae254f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325bab3-0ed4-4db9-ba68-281d669f0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91049c19-09f2-4c14-aad3-e7e8335f5991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b7ab9d3-1c39-406e-a804-9b00a4231de1",
   "metadata": {},
   "source": [
    "### Question 1 - 4 min Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785cecb-e202-4787-94c0-cb3d4813dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'How many disciplines does electrical engineering overlaps with as shown in the slides in the video'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faeea6e-557f-40d6-816a-b2a9c00e07ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better results, we recommend to prompt the model in the following format\n",
    "prompt = f\"USER: <video>\\n{question}? ASSISTANT:\"\n",
    "inputs = processor(text=prompt, videos=video, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db168e1-a6dc-410f-8f42-57e64c81674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13717c66-f28c-4057-ba79-aa89e5df2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "out = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=256,\n",
    "    output_hidden_states =True,\n",
    "    return_dict_in_generate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b46e54-1389-46df-823e-3bd08977f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = processor.batch_decode(\n",
    "    out['sequences'], \n",
    "    skip_special_tokens=True, \n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "generated_text[0].split('ASSISTANT:')[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0c4841-88ee-4e6c-ad87-103128dc2fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a862386b-7967-497e-a94f-a496de8656c9",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7adda2-9dfe-4e7a-8542-57303cf8ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better results, we recommend to prompt the model in the following format\n",
    "prompt = \"USER: <video>\\nWhat colour the sample turn to after adding vinegar?? ASSISTANT:\"\n",
    "inputs = processor(text=prompt, videos=video, return_tensors=\"pt\")\n",
    "inputs = inputs.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1fa32-e7f7-457e-8b0c-39805ecb0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "out = model.generate(**inputs, max_new_tokens=256)\n",
    "processor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d3e8d0-be7f-4bbc-aa69-648a3ba2700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8218b-d996-4dfe-8843-20fd30e586ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7535ab53-cefe-44fd-99aa-fbcf1d811360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db5dacd6-bace-48da-9d1e-64804e55d356",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d086608a-a917-400c-b47f-febbf5786cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better results, we recommend to prompt the model in the following format\n",
    "prompt = \"USER: <video>\\nWhat is the nature of the pickles made: Acidic or basic? ASSISTANT:\"\n",
    "inputs = processor(text=prompt, videos=video, return_tensors=\"pt\")\n",
    "inputs = inputs.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb29f87-1465-4ede-bdfa-648ecbc13375",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "out = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=256,\n",
    "    output_hidden_states =True,\n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "processor.batch_decode(out['sequences'], skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d13a673-7a9f-4447-bb78-b42420da2157",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f416a6a-01c7-44f9-87d2-6acc4950156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_activations(out, inputs, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813643f4-2d3c-41ef-aff0-36e30f26c997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715506e3-5c80-4659-9321-0b660abcf8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92d913cf-ddf4-4e87-b255-1103157c1f27",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785121b8-59ba-4eb6-9c07-5576ca86d71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better results, we recommend to prompt the model in the following format\n",
    "prompt = \"USER: <video>\\nWill the vehicles in the video be safe to eat? ASSISTANT:\"\n",
    "inputs = processor(text=prompt, videos=video, return_tensors=\"pt\")\n",
    "inputs = inputs.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9330b47-814c-4698-bb6a-21d81fc5f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "out = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=256,\n",
    "    output_hidden_states =True,\n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "processor.batch_decode(out['sequences'], skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d13e88-5232-4ba1-8a09-cf973f0c7d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3877de1-145f-4692-9a14-3fc78689cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f116db3-26ab-4b1a-9451-064c81797299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b85cd04a-cfe6-4bab-87db-21eb65f807c8",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c310f90-d37f-4ffe-8c0c-eeb36b741d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better results, we recommend to prompt the model in the following format\n",
    "prompt = \"USER: <video>\\Which vehicle is present in the video? ASSISTANT:\"\n",
    "inputs = processor(text=prompt, videos=video, return_tensors=\"pt\")\n",
    "inputs = inputs.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b1c87-f6b2-48cf-9327-3f1c5598b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "out = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=256,\n",
    "    output_hidden_states =True,\n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "processor.batch_decode(out['sequences'], skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee2977-e3d9-4dea-b262-82e20d36677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97495bd3-2493-45ee-ac5d-285b162ac459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8a86ede-0d3a-4d31-9a88-8afc6058b84d",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a807fe-562c-4a98-be89-026138cf2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better results, we recommend to prompt the model in the following format\n",
    "prompt = \"USER: <video>\\nWhat is the nature of the doughnuts made: Acidic or basic? ASSISTANT:\"\n",
    "inputs = processor(text=prompt, videos=video, return_tensors=\"pt\")\n",
    "inputs = inputs.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fce00d-1e1c-42b0-90e5-8256289cb279",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "out = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=256,\n",
    "    output_hidden_states =True,\n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "processor.batch_decode(out['sequences'], skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e90fe-204e-4d5b-82c4-8683b23d7fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963ba5b-c568-474b-9081-c8a028e33451",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_activations(out, inputs, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137f02d-c370-4c6e-929c-d7b7643a4fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980325b3-154a-4fca-b37c-ebac4ee22f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
