{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970ebaad-687f-491a-875f-018d5808b655",
   "metadata": {},
   "source": [
    "This model will require ffmpeg package installation in the conda-env, system wide installation is not required.\n",
    "\n",
    "Use 'pip install python-ffmpeg' wrapper package for installation.\n",
    "\n",
    "Right now runs in the stable_env environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d219e49-ba18-484d-941e-bc5bd6837fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stable_env\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(os.getenv(\"CONDA_DEFAULT_ENV\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c4d77e-573a-4348-8853-044336b06b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, AutoModel, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0836f84e-8cbe-4c2e-9cb5-fd59daefc8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"DAMO-NLP-SG/VideoLLaMA3-7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa2d48e-0a63-49f0-a9e3-cf0e70b5b835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c2168c7-8da5-42a9-9d7d-7ee8c1b6733d",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8db14c92-bd9f-46c2-bab1-038030a413fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c077995804f4450c835dda1eb604ad8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18cac88e-0a57-4434-ad52-1e8135ff44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "995bb0c7-202b-4e67-bca1-c457b8f8fb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Videollama3Qwen2Model(\n",
       "  (embed_tokens): Embedding(152064, 3584)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x Qwen2DecoderLayer(\n",
       "      (self_attn): Qwen2Attention(\n",
       "        (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "        (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "        (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "        (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "      )\n",
       "      (mlp): Qwen2MLP(\n",
       "        (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "        (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "        (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "  (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  (vision_encoder): Videollama3VisionEncoderModel(\n",
       "    (embeddings): Videollama3VisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "    )\n",
       "    (encoder): Videollama3VisionTransformerEncoder(\n",
       "      (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-26): 27 x Videollama3VisionEncoderLayer(\n",
       "          (self_attn): VisionFlashAttention2(\n",
       "            (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Videollama3VisionMLP(\n",
       "            (activation_fn): GELUTanh()\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (mm_projector): MlpGeluProjector(\n",
       "    (readout): Sequential(\n",
       "      (0): Linear(in_features=1152, out_features=3584, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d14d7b2-97aa-4f26-b5b6-354cb5a2a345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c0b784d-a8c2-4172-9060-09b89506fa3e",
   "metadata": {},
   "source": [
    "### Set Question & Video Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd39d863-f558-4719-825e-32103b30fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '/home/aritrad/MSR-Project/random/insertion_sort.mp4'\n",
    "question =  'How many element is present in the array shown?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319e524-f844-4113-8fab-f7d40c6d11d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62c5c4b0-5a71-4c4a-a4e9-37b30e09593f",
   "metadata": {},
   "source": [
    "### Create Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d6baecd-32bf-441b-ba02-b7bf7b752e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video conversation\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a vision-language model analyzing lecture videos. \"\n",
    "            \"You must rely strictly on visual evidence from the video frames.\\n\\n\"\n",
    "            \"Follow these steps:\\n\"\n",
    "            \"1. Carefully examine the video frames.\\n\"\n",
    "            \"2. Perform OCR on any readable text appearing on slides, blackboards, or written material.\\n\"\n",
    "            \"3. Explicitly list the extracted text before reasoning.\\n\"\n",
    "            \"4. Answer the question using ONLY the extracted visual information.\\n\"\n",
    "            \"5. If the required information is not visible or readable, say \"\n",
    "            \"\\\"The answer cannot be determined from the video.\\\" Do NOT guess.\\n\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\", \n",
    "                \"video\": \n",
    "                {\n",
    "                    \"video_path\": video_path, \n",
    "                    \"fps\": 1, \n",
    "                    \"max_frames\": 240\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \"text\": question\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed8bc8-d1e2-4609-8bd4-aa319fd96a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e1ab921-0cb6-477a-b176-49def08389d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(conversation=conversation, return_tensors=\"pt\")\n",
    "inputs = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dcfaaa4-ca04-4235-891b-60960ab0ad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"pixel_values\" in inputs:\n",
    "    inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd865b5-1d69-4bbc-8733-152e29e20045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ede12ade-5649-4905-8c64-b90153e0254d",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd1764b-f4ff-4184-a25b-63c209f6c9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array has 10 elements.\n"
     ]
    }
   ],
   "source": [
    "output_ids = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=512, \n",
    "    do_sample=False\n",
    ")\n",
    "response = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b240799f-c4a1-49db-92d1-64add2fcba12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96507abf-b846-4284-a162-fe7897b9ed5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
