{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb248bf-4a7b-4878-8a6c-da4252c0073b",
   "metadata": {},
   "source": [
    "Ref: https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl3\n",
    "\n",
    "Run this in vid_env environment. This models works with transformers ver 4.37.2 to 4.47.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "828d8350-520d-4550-9e7b-7654a54607ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "608d54f2-ec2b-47a3-948b-498f9e991cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stable_env\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"CONDA_DEFAULT_ENV\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5fcb3f-7d57-42df-b9f5-d86c7fc8b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6baf553-5938-4f4e-9ed3-026dc3e67472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "from transformers import AutoTokenizer, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e156d4d7-dd45-4e8d-b612-5a177dcbc3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from decord import VideoReader, cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377654b9-27ac-4277-a83c-2493471b3e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "450c6ddf-ffb6-429f-99bc-04f19f970aa9",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc46923d-70d0-4572-97e8-68d2353811ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'mPLUG/mPLUG-Owl3-7B-240728'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4202e5a9-21f6-4a37-a754-5bf64f0e62c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "HyperQwen2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use flash_attn rotary\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.half, \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "processor = model.init_processor(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9706c15-8fe2-4e23-adb5-960161751e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f00a12a-8050-4496-83a7-731073389a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mPLUGOwl3Model(\n",
       "  (language_model): HyperQwen2ForCausalLM(\n",
       "    (model): HyperQwen2Model(\n",
       "      (embed_tokens): Embedding(151851, 3584)\n",
       "      (layers): ModuleList(\n",
       "        (0): HyperQwen2DecoderLayer(\n",
       "          (self_attn): HyperQwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "            (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            (rotary_emb_core): RotaryEmbedding()\n",
       "            (v_kv_proj): Linear(in_features=3584, out_features=1024, bias=True)\n",
       "            (gate_proj): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Sigmoid()\n",
       "            )\n",
       "            (v_core_attention_sdpa): ScaleDotProductAttention()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "        (1-7): 7 x HyperQwen2DecoderLayer(\n",
       "          (self_attn): HyperQwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "            (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            (rotary_emb_core): RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "        (8): HyperQwen2DecoderLayer(\n",
       "          (self_attn): HyperQwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "            (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            (rotary_emb_core): RotaryEmbedding()\n",
       "            (v_kv_proj): Linear(in_features=3584, out_features=1024, bias=True)\n",
       "            (gate_proj): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Sigmoid()\n",
       "            )\n",
       "            (v_core_attention_sdpa): ScaleDotProductAttention()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "        (9-15): 7 x HyperQwen2DecoderLayer(\n",
       "          (self_attn): HyperQwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "            (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            (rotary_emb_core): RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "        (16): HyperQwen2DecoderLayer(\n",
       "          (self_attn): HyperQwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "            (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            (rotary_emb_core): RotaryEmbedding()\n",
       "            (v_kv_proj): Linear(in_features=3584, out_features=1024, bias=True)\n",
       "            (gate_proj): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Sigmoid()\n",
       "            )\n",
       "            (v_core_attention_sdpa): ScaleDotProductAttention()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "        (17-23): 7 x HyperQwen2DecoderLayer(\n",
       "          (self_attn): HyperQwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "            (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            (rotary_emb_core): RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "        (24): HyperQwen2DecoderLayer(\n",
       "          (self_attn): HyperQwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "            (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            (rotary_emb_core): RotaryEmbedding()\n",
       "            (v_kv_proj): Linear(in_features=3584, out_features=1024, bias=True)\n",
       "            (gate_proj): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Sigmoid()\n",
       "            )\n",
       "            (v_core_attention_sdpa): ScaleDotProductAttention()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "        (25-27): 3 x HyperQwen2DecoderLayer(\n",
       "          (self_attn): HyperQwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "            (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            (rotary_emb_core): RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=3584, out_features=151851, bias=False)\n",
       "  )\n",
       "  (vision_model): SiglipVisionTransformer(\n",
       "    (embeddings): SiglipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "      (position_embedding): Embedding(729, 1152)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-26): 27 x SiglipEncoderLayer(\n",
       "          (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attn): SiglipAttention(\n",
       "            (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): GELUTanh()\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): SiglipMultiheadAttentionPoolingHead(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)\n",
       "      )\n",
       "      (layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): SiglipMLP(\n",
       "        (activation_fn): GELUTanh()\n",
       "        (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "        (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vision2text_model): Linear(in_features=1152, out_features=3584, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78791fc6-b2fa-4134-8c0b-1192fe28cd27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1023c4d8-94a4-4aa9-9838-9c659b47ca39",
   "metadata": {},
   "source": [
    "### Prepare Chat Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d762ba-6097-4e73-8dc3-dc1852981919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_message(prompt):\n",
    "\n",
    "    message = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a vision-language model analyzing lecture videos. \"\n",
    "                \"You must rely strictly on visual evidence from the video frames.\\n\\n\"\n",
    "                \"Follow these steps:\\n\"\n",
    "                \"1. Carefully examine the video frames.\\n\"\n",
    "                \"2. Perform OCR on any readable text appearing on slides, blackboards, or written material.\\n\"\n",
    "                \"3. Explicitly list the extracted text before reasoning.\\n\"\n",
    "                \"4. Answer the question using ONLY the extracted visual information.\\n\"\n",
    "                \"5. If the required information is not visible or readable, say \"\n",
    "                \"\\\"The answer cannot be determined from the video.\\\" Do NOT guess.\\n\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"<|video|> {prompt}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\"\n",
    "        }\n",
    "    ]\n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4023e1-fa76-4cef-88ed-cb55d3bc6525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e29b5580-fb69-4412-be27-23699009eac5",
   "metadata": {},
   "source": [
    "### Question for Random Test:\n",
    "\n",
    "From chemistry.mp4\n",
    "\n",
    "question = 'What was the capacity of the beaker they used in the experiment?'\n",
    "\n",
    "question = 'What was the melting pointed shown in the experiment?'\n",
    "\n",
    "question = 'Did they use Potassium chlorate in the experiment?'\n",
    "\n",
    "question = 'Did they use water in the whole experiment?'\n",
    "\n",
    "question = 'What are the compounds being used here, list them'\n",
    "\n",
    "------------------------------------------------------------------------------------------------------\n",
    "\n",
    "From insertion_sort.mp4\n",
    "\n",
    "question = 'Which sorting algorithm is being performed in the video?'\n",
    "\n",
    "question = 'How many element is present in the array shown ?'\n",
    "\n",
    "question = 'What is the runtime of the algorithm as explained by the professor?'\n",
    "\n",
    "question = 'Write the elements present in the array the professor is explaining?'\n",
    "\n",
    "------------------------------------------------------------------------------------------------------\n",
    "\n",
    "From 15min-video.mp4\n",
    "\n",
    "question = 'UML is adapted by which group and in which year?'\n",
    "\n",
    "question = 'How many views of a system are there and what are they?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b7040-c47d-4758-acab-5b47e2c66659",
   "metadata": {},
   "source": [
    "### Load Test Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66c3fb-ef6a-4226-886e-36c772d7c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = ['/home/aritrad/MSR-Project/samples/15min-video.mp4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9b520d-2a4d-4a2b-bf7c-6d61847d6aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_FRAMES=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bffb4ae-04a7-47b0-b282-194f9f59ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_video(video_path):\n",
    "    \n",
    "    def uniform_sample(l, n):\n",
    "        gap = len(l) / n\n",
    "        idxs = [int(i * gap + gap / 2) for i in range(n)]\n",
    "        return [l[i] for i in idxs]\n",
    "\n",
    "    vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    sample_fps = round(vr.get_avg_fps() / 1)  # FPS\n",
    "    frame_idx = [i for i in range(0, len(vr), sample_fps)]\n",
    "    if len(frame_idx) > MAX_NUM_FRAMES:\n",
    "        frame_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES)\n",
    "    frames = vr.get_batch(frame_idx).asnumpy()\n",
    "    frames = [Image.fromarray(v.astype('uint8')) for v in frames]\n",
    "    #print(frames)\n",
    "    print('num frames:', len(frames))\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13998d2f-7018-4fac-84c9-6c0710925043",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = [encode_video(_) for _ in videos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ade61-5e23-4a47-b68e-b5b51783f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames[0][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2cc40-5ef0-4f7e-afa0-bd0ef79951b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66c6b445-f6f3-4b4d-808f-65e6f6daadb5",
   "metadata": {},
   "source": [
    "### Generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eaa39d-1e6c-4dc6-abcf-4bdc2c310a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'How many views of a system are there and what are they?'\n",
    "message = create_chat_message(question)\n",
    "inputs = processor(message, images=None, videos=video_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba02c9-7041-4f47-a858-f7539f293b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.to('cuda')\n",
    "inputs.update({\n",
    "    'tokenizer': tokenizer,\n",
    "    'max_new_tokens':512,\n",
    "    'decode_text':True,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749e050-7582-4749-950a-8d4d74cfa6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "answer = model.generate(**inputs)\n",
    "print(answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc312d-457e-4273-8a46-2885a9123a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636a66b-0631-4367-8875-39c8eb20dc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c82d32-749f-4ced-8e5f-12c23f92a86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef834d9-f0cd-453a-bd9f-2badb520df7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
