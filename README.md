# ğŸ“ Lecture-RAG for Educational Videos  
### Grounded Video Question Answering with Self-Refinement

> **TL;DR**  
> Lecture videos (slides + blackboard + face cam) break standard Video-LLMs.  
> Lecture-RAG is a grounding-aware Video-RAG framework that reduces hallucinations and supports algorithmic reasoning in educational videos.

---

## ğŸš¨ Motivation

Most Video-Language Models are designed for:
- short clips,
- natural scenes,
- action-centric benchmarks.

**Lecture videos are fundamentally different.**

They contain:
- dense slides and equations,
- handwritten blackboard content,
- algorithm pseudocode,
- long durations with sparse visual change.

As a result, existing systems fail in two major ways:

### âŒ Hallucination
Models answer confidently from **prior knowledge**, even when:
- the relevant slide is not sampled,
- the video is blank or irrelevant.

### âŒ Over-Abstention
When strict grounding is enforced, models respond with:

> *â€œThe answer cannot be determined from the video.â€*

â€”even when answers are **logically derivable** from steps or equations shown in the lecture.

---

## ğŸ’¡ Key Insight

> **Grounding in educational videos is not binary.**

Lecture QA requires distinguishing between:

| Grounding Type | Example | Action |
|---------------|--------|--------|
| **Explicit** | â€œWhat is written on the slide?â€ | Answer |
| **Derivable from steps** | â€œWhy initialize keys to âˆ in Primâ€™s algorithm?â€ | Answer |
| **Theoretical / external** | â€œWhy does Prim always produce an MST?â€ | Abstain |

Most existing approaches collapse everything into *supported vs unsupported*, which breaks algorithmic reasoning.

---

## ğŸ§  What is Lecture-RAG?

**Lecture-RAG** is a **Grounding-Aware Video RAG framework** tailored for educational videos.

It combines:
- OCR-based evidence extraction,
- query-aware retrieval over lecture content,
- iterative self-refinement with grounding feedback.

The goal is to:
- prevent hallucinations,
- avoid unnecessary abstention,
- support algorithmic and procedural reasoning.

---

## ğŸ§© Core Components

### 1ï¸âƒ£ OCR-First Evidence Modeling
- OCR is treated as **primary grounding evidence**.
- The model is restricted to:
  - OCR text
  - clearly visible visual content
- External knowledge is disallowed unless **derivable from shown steps**.

---

### 2ï¸âƒ£ Query-Aware OCR Retrieval
- OCR is extracted from uniformly sampled frames.
- A **hybrid retrieval module** (semantic + lexical) selects OCR segments relevant to the question.
- Removes noise from:
  - instructor bios,
  - course outlines,
  - unrelated slides.

---

### 3ï¸âƒ£ Grounding-Aware Self-Refinement
Inspired by **SELF-REFINE**, adapted to multimodal grounding.

Each iteration consists of:
1. Answer generation
2. Grounding feedback classification
3. Answer refinement

Answers are classified as:
- `SUPPORTED`
- `DERIVABLE_FROM_STEPS`
- `PARTIALLY_SUPPORTED`
- `UNSUPPORTED`

This enables explanation-based answers without hallucination.

---

### 4ï¸âƒ£ Robust Failure Handling
- On black-screen or irrelevant videos, the system **correctly abstains**.
- Prevents confident but ungrounded outputs.

---

## ğŸ§  Architecture Overview

```text
Video
â”œâ”€ Uniform frame sampling (OCR-oriented)
â”œâ”€ OCR extraction
â”œâ”€ Query-aware OCR retrieval
â”œâ”€ Grounded Answer Generation (Qwen2.5-VL / LLaVA / mPLUG-Owl)
â”œâ”€ Grounding Feedback
â””â”€ Iterative Self-Refinement
â†“
Final Grounded Answer
```
---

## ğŸ“ Repository Structure

```text
LectureRAG/
â”œâ”€â”€ framework.py                        # Main pipeline (OCR + retrieval + refinement)
â”œâ”€â”€ hybrid_search.py                    # Query-aware OCR retrieval
â”œâ”€â”€ run_ocr.py                          # OCR execution script
â”œâ”€â”€ nanonetOCR.py                       # OCR wrapper
â”œâ”€â”€ self_refine_framework_llavaNext.py  # LLaVA-NeXT variant
â”œâ”€â”€ self_refine_framework_mPlugOwl.py   # mPLUG-Owl variant
â”œâ”€â”€ self_refine_framework_qwen2_5.py    # Qwen2.5-VL variant
â”œâ”€â”€ frameworkocr_*.pkl                  # Cached OCR outputs
â”œâ”€â”€ sampled_frames.jpeg                 # Example sampled frames
â”œâ”€â”€ samples/                            # Sample lecture videos
â”œâ”€â”€ README.md
```
---

## ğŸš€ How to Run

```bash
python self_refine_framework_qwen2_5.py
```

### Requirements

*   GPU compatible with Qwen2.5-VL / LLaVA-NeXT / mPLUG-Owl
*   Python â‰¥ 3.9
*   transformers, torch, decord, opencv
*   NanoNet OCR (or compatible OCR backend)

## ğŸ“š Inspiration & Related Work

This project is inspired by:

*   **SELF-REFINE**: Iterative Refinement with Self-Feedback, NeurIPS 2023
*   **Video-RAG**: Visually-aligned Retrieval-Augmented Long Video Comprehension, NeurIPS 2025

Lecture-RAG adapts these ideas to the educational video domain, introducing grounding-aware refinement and OCR-centric retrieval.

## ğŸ”® Future Work

*   ğŸ”Š Automatic Speech Recognition (ASR) integration
*   ğŸ¯ Fully query-aware frame sampling
*   ğŸ“Š Evaluation on educational video QA benchmarks
*   ğŸ§  Temporal reasoning across slide transitions

## ğŸ“Œ Takeaway

Lecture videos are not just another video domain.
They require OCR-aware grounding, step-based reasoning, and careful self-refinement.