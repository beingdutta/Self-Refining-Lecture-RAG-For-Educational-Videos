{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85a61ca4-8708-41a6-acde-c9678208c478",
   "metadata": {},
   "source": [
    "Reference: https://huggingface.co/nanonets/Nanonets-OCR-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed0cb87-50c2-407e-aea3-0cca1c86f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02df4ff7-c359-4c1d-a8be-17c5cc212325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stable_env\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(os.getenv(\"CONDA_DEFAULT_ENV\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df1766-69b4-450c-99cf-9e6591d5bf7d",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "636550fb-cdf9-4780-b774-dbd0d34f65ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"nanonets/Nanonets-OCR-s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94390147-a733-4baf-8791-14c66de1d241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13e647f820443f0a7c8a43243f4c8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1b68c120124e7e93a4f5aa2a5e1100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_path, \n",
    "    dtype=\"auto\", \n",
    "    device_map=\"auto\", \n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3e61e62-c0c7-4a83-9c3c-11bf6bc4d466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4b8942-4415-4e4d-a141-cd9863885040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VLForConditionalGeneration(\n",
       "  (model): Qwen2_5_VLModel(\n",
       "    (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "      (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (attn): Qwen2_5_VLVisionAttention(\n",
       "            (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "            (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (mlp): Qwen2_5_VLMLP(\n",
       "            (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "            (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "            (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (merger): Qwen2_5_VLPatchMerger(\n",
       "        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (language_model): Qwen2_5_VLTextModel(\n",
       "      (embed_tokens): Embedding(151936, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2_5_VLDecoderLayer(\n",
       "          (self_attn): Qwen2_5_VLAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b172b05-5e98-42eb-8626-3e800ad03f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51df6431-8181-4dc6-904b-244a83b8df2f",
   "metadata": {},
   "source": [
    "### Load the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e7c5e-144a-43d1-9e9c-13e10e6721ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/aritrad/test_images/electric.PNG\"\n",
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8027c77-e9dd-40a0-b818-b7cf918e74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using ☐ and ☑ for check boxes.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2487d3d2-3962-4f98-ac43-c101aebdf308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3aa02aa-7132-4318-93c7-3f96b272090e",
   "metadata": {},
   "source": [
    "### Create Chat Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51582bbc-2a22-41fb-9331-be932bf54eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\", \"image\": f\"file://{image_path}\"},\n",
    "        {\"type\": \"text\", \"text\": prompt},\n",
    "    ]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07b3d5-bbcf-4d77-bef2-abd0f07a3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = processor(text=[text], images=[image], padding=True, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e536089-1c28-4406-9de7-59da5213ad9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb2d02a8-02dc-4fac-a1fd-7e5d36fc0320",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c020833d-b9ac-416d-876b-32390a37048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "output_ids = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=1024, \n",
    "    do_sample=True,  # <--- Change this to True\n",
    "    temperature=0.7  # (Optional) Set your desired temp\n",
    ")\n",
    "\n",
    "generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0578f06-8c5f-4bb0-97ff-e6395e9502bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "print(output_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0688cfd4-5610-4d66-a4df-2b8a2e111525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
